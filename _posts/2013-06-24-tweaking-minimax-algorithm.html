---
layout: article
title: Tweaking the Minimax Algorithm
date: '2013-06-24T21:03:00.001-07:00'
author: aml
tags: 
modified_time: '2013-06-24T21:05:52.280-07:00'
blogger_id: tag:blogger.com,1999:blog-1730093662423317857.post-4336296068131529205
blogger_orig_url: https://immutablearlandis.blogspot.com/2013/06/tweaking-minimax-algorithm.html
---

So, I've been working on this <a href="https://github.com/arlandism/tic_tac_toe" target="_blank">Tic Tac Toe</a> game for quite awhile now. While I've had the minimax algorithm implemented that entire time, there were still some flaws lurking.<br /><br />First of all, my application version only worked when the computer was 'o'. Thinking back, I'm not sure how I ended up with that bug. I think it was because I didn't completely understand what I was doing. But, I didn't have that problem once I started the TDD version at 8th Light.<br /><br />The next problem I encountered was that the algorithm ran too slow. It was taking around 20 seconds to run less than fifty tests. That's extremely slow for anyone unfamiliar. To remedy that, I implemented <a href="http://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning" target="_blank">alpha-beta pruning</a>&nbsp;and that sped it up quite a bit. Oddly, after a bit of refactoring, it slowed down again, but nowhere near it's original tortoise-like state.<br /><br />The most recent problem I faced was that the algorithm was, shall we say "too smart"! I found a very specific sequence of moves that would trigger some odd behavior. Eventually, I figured out that at certain stages of the game the algorithm knew that it could win no matter how it played. It would pick moves that would guarantee a game win, but wouldn't win immediately. So, for instance if it had two in a row and it's opponent wasn't even close to winning, it would set up a two-way win, just to be sure, or ruthless, or to toy with its opponent (What a mean algorithm!). Anyway, a bright apprentice by the name of <a href="http://andrewzures.blogspot.com/" target="_blank">Andrew</a> helped me devise a solution. It was basically all a math trick. I have three terminal states: win: 1, loss: -1, tie: 0. With only these states, I couldn't figure out how to use any other metric to help the AI determine that an earlier win would be better than a later one. Then I saw that Andrew's code made use of a helpful depth parameter that, in conjunction with a max-depth constant, helped the computer narrow down the best choice.<br /><br />So, we have a max-depth constant, say: 100. And we have a depth parameter that actually measures how far you've gone down the game tree and we'll start it at 1. Then, at the terminal states, you can multiply your terminal state (the win, loss, or tie score) by the max-depth constant, and then divide that by your current depth. The lower your depth, the larger the corresponding score, which means earlier moves are favored over later ones. I've included some sample code below as a reference. Also, the max-depth constant functions as an early stopping point, not just an arbitrary multiplier. *NOTE* It is not a complete minimax implementation. It's just the code representing the above point. And I don't use 100 as my constant.<br /><br /><br /><pre class="brush:python" name="code">MAX_DEPTH = 100<br />def minimax(space,board,current_player,depth=1):<br />    if board.terminal_state():<br />        return (MAX_DEPTH * board.evaluation_function()) / depth<br />    # Further implementation here<br />    # Increment depth with each recursive call<br /></pre>There may indeed be more clever ways to do this, but right now this functions well for me.  
