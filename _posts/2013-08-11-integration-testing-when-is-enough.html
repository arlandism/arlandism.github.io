---
layout: article
title: 'Integration Testing: When is Enough, Enough?'
date: '2013-08-11T18:25:00.002-07:00'
author: aml
tags: 
modified_time: '2013-08-11T18:25:20.203-07:00'
blogger_id: tag:blogger.com,1999:blog-1730093662423317857.post-3628803150043314082
blogger_orig_url: https://immutablearlandis.blogspot.com/2013/08/integration-testing-when-is-enough.html
---

I'm working on a story right now that forces me to modify the API to my tic tac toe application. The story itself isn't too difficult; it's already feature-complete, it just needs to be available through the API. What really got me thinking were the test cases. There are already plenty of tests in place for the actual feature, but there aren't any integration tests that show the feature being exercised through the API. My first thought was just to write another integration test. But, I'm starting to listen harder to my programmer instincts and they started going off like crazy. I have around 125 test cases, with around 3 or 4 of those as integration tests. But, those 3 or 4 integration tests are pretty lengthy. And rightfully so, they actually need to talk to the API and rely on the actual production stuff being used. But, do I really want to write a new integration test every time the API can do something new, even if the underlying code isn't changing? I think the answer here is no, but that brings me to another question: how can I enable new API features without writing integration tests for each?<br /><br /><a href="http://blog.thecodewhisperer.com/2010/10/16/integrated-tests-are-a-scam/" target="_blank">This guy</a> has very strong opinions on the matter. I certainly see where he's coming from. He basically makes the same argument I just mentioned: to test every deviation of every feature would require a gargantuan test suite plus an army of integration test writers. But, I didn't see any solutions mentioned on his blog. Certainly, you want to minimize the number of dependencies that a class has. But, there's no arguing that there are times when you need a class to work in conjunction with another. One alternative that immediately comes to mind is to pass in a mock class and make assertions about how the class that's being tested works with the mock. Sure, that tests delegation, but that's not true integration. Should I use mocks in true integration tests? If so, there's no way to truly hit the lowest rungs of the application's ladder. If not, how do I account for those black-box services that my application needs to hit, like databases and network connections? It seems smelly, wasteful even, to write tons of fake data to a production database just to test all the boundaries.<br /><br />So, the question becomes: How many integration tests should you write? Should they be as thorough as your unit tests? If not, how do you know when you've written enough?
